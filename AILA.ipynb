{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blublahblah123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install gpg.exe file to decrypt the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tar = tarfile.open(\"aila20-task1.tar.gz\", \"r:gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tar.extractall(r'C:\\Users\\Aditi\\AILA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is how I am planning to proceed\n",
    "1. Data Cleaning\n",
    "2. To find the relavant case docs\n",
    "    2a. Deep Bayesian Learning\n",
    "        Method : Generative Model\n",
    "           1a. Document: Distribution over a topic\n",
    "           1b. Topic : Distribution over a word\n",
    "    2b. Bayesian Deep Learning\n",
    "        Method : Uncertinity Model\n",
    "            2a. Aleatoric uncertinity\n",
    "            2b. Epistemic uncertinity\n",
    "3. References\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "\n",
    "ProblemLink: https://sites.google.com/view/aila-2020/dataset-evaluation-plan?authuser=0\n",
    "\n",
    "Google Group For Quiries: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!topic/aila-fire/EwDlG4cRwyY\n",
    "\n",
    "\n",
    "The zip file consists of the following documents:\n",
    "\n",
    "1. Query_doc.txt -- contains the 50 queries, i.e., description of legal situations\n",
    "2. Object_casedocs -- contains 2,914 prior case documents, some of which are relevant to the queries (to be used for Task 1).\n",
    "3. Object_statutes -- contains the title and textual description of 197 statutes (to be used for Task 2)\n",
    "4. Gold standard annotations for relevant precedents and\n",
    "5. README.txt file that specifies the formats of the other files / folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A : Identifying relevant prior cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import gensim #Gensim is billed as a Natural Language Processing package that does ‘Topic Modeling for Humans’\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries in the dataset:  50\n"
     ]
    }
   ],
   "source": [
    "with open ('C:/Users/Aditi/AILA/dataset/Query_doc.txt', 'r') as f:\n",
    "    all_lines = f.read().splitlines() \n",
    "    \n",
    "print(\"Total queries in the dataset: \", len(all_lines)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total casedocs in the train dataset:  2914\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/Aditi/AILA/dataset/Object_casedocs/'\n",
    "count = 0\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "    count = count + 1\n",
    "print(\"Total casedocs in the train dataset: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Masud Khan v State Of Uttar Pradesh\\nSupreme Court of India\\n\\n26 September 1973\\nWrit Petition No.', '117 of 1973\\nThe Judgment was delivered by : A. Alagiriswami, J.']\n",
      "['Prabhakaran Nair, Etc.', 'v State Of Tamil Nadu And Ors.', 'Supreme Court of India\\n\\n3 September 1987\\nWrit Petition No.', '506 of 1986\\nThe Judgment was delivered by: Sabyasachi Mukharji, J.']\n",
      "['Hiten P. Dalal v Bratindranath Banerjee\\nSupreme Court of India\\n\\n11 July 2001\\nAppeal (Cr.)', '688 of 1995\\nThe Judgment was delivered by : Ruma Pal, J\\n1.']\n",
      "['Ashok Kumar and Others v State of Tamil Nadu\\nSupreme Court of India\\n\\n5  May  2006\\nAppeal (Crl.)', '1533 of 2004\\nThe Judgment was delivered by : S. B. Sinha, J.']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bfeafba0e5fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'The Judg'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/Aditi/AILA/dataset/Object_casedocs/'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "j = 0 ### iterate for a column\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "    with open (filename, 'r') as f:\n",
    "        sentenses = [] ### Store all the sentences into a list\n",
    "        tokens = sent_tokenize(f.read())\n",
    "        \n",
    "        for line in tokens:\n",
    "            sentenses.append(line)\n",
    "    \n",
    "        indices = [i for i, s in enumerate(sentenses) if 'The Judg' in s]\n",
    "    \n",
    "        df.loc[j,'Basic Info'] = sentenses[0:indices[0]+1]\n",
    "        df.loc[j,'Case Details' ] = sentenses[indices[0]+1:len(sentenses)]\n",
    "        \n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-716dbea8fe34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'The Judg'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Basic Info'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentenses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    364\u001b[0m                         new_indexer = convert_from_missing_indexer_tuple(\n\u001b[0;32m    365\u001b[0m                             indexer, self.obj.axes)\n\u001b[1;32m--> 366\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m                         raise ValueError('Must have equal len keys and value '\n\u001b[0m\u001b[0;32m    612\u001b[0m                                          'when setting with an iterable')\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "with open ('C:/Users/Aditi/AILA/dataset/Object_casedocs/C1.txt', 'r') as f:\n",
    "    sentenses = [] ### Store all the sentences into a list\n",
    "    tokens = sent_tokenize(f.read())\n",
    "        \n",
    "    for line in tokens:\n",
    "        sentenses.append(line)\n",
    "    \n",
    "    indices = [i for i, s in enumerate(sentenses) if 'The Judg' in s]\n",
    "    \n",
    "    df.loc[j,'Basic Info'] = sentenses[0:indices[0]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('C:/Users/Aditi/Similarity/workdir',tf_idf[corpus],num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C101.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C1149.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C14.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C1512.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C169.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C1714.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C1777.txt 190\n",
      "C:/Users/Aditi/AILA/dataset/Object_casedocs\\C2356.txt 190\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    avg_sims = [] # array of averages\n",
    "    file2_docs = []\n",
    "    with open (filename, 'r') as f:\n",
    "        tokens = sent_tokenize(f.read())\n",
    "        for line in tokens:\n",
    "            file2_docs.append(line)\n",
    "\n",
    "    # for line in query documents\n",
    "    for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "    # calculate total average\n",
    "    total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "    if total_avg >= 50:\n",
    "        print(filename, percentage_of_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total average\n",
    "total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "# round the value and multiply by 100 to format it as percentage\n",
    "percentage_of_similarity = round(float(total_avg) * 100)\n",
    "# if percentage is greater than 100\n",
    "# that means documents are almost same\n",
    "if percentage_of_similarity >= 100:\n",
    "    percentage_of_similarity = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.478158907682607"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning(Remove names other important stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure of similarity can be qualitative and/or quantitative. In qualitative, the assessment is done against \n",
    "subjective criteria such as theme, sentiment, overall meaning, etc. \n",
    "In the quantitative, numerical parameters such as length of the document, number of keywords, common words, etc. are compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep Bayesian Learning\n",
    "\n",
    "Bayesian Deep Learning ( Tensorflow Probablilty)\n",
    "aleatoric and epistemic uncertiny\n",
    "\n",
    "Entitie\n",
    "Topics\n",
    "\n",
    "Dirichlet algorithm\n",
    "\n",
    "Generative Model\n",
    "Document: Distribution over a topic\n",
    "Topic : Distribution over a word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
